---
title: "Smoothing Temperature Data"
author: "Steve Bischoff"
date: "2023-12-19"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r include=FALSE, warning=FALSE}
suppressWarnings(library(ggplot2))
suppressWarnings(library(reshape2))
suppressWarnings(library(tseries))
suppressWarnings(library(tidyverse))
suppressWarnings(library(ks))
suppressWarnings(library(ramify))
suppressWarnings(library(pracma))
```

```{r include=FALSE}
data_aus = read.csv('data/daily_summaries_austin.csv', header=TRUE)
data_aus$DATE = as.Date(data_aus$DATE)
```

```{r include=FALSE}
data_sd = read.csv('data/daily_summaries_sandiego.csv', header=TRUE) %>% drop_na(TMAX)
data_sd$DATE = as.Date(data_sd$DATE)
```

```{r include=FALSE}
data_mn = read.csv('data/daily_summaries_minneapolis.csv', header=TRUE)
data_mn$DATE = as.Date(data_mn$DATE)
```

```{r include=FALSE}
data_bt = read.csv('data/daily_summaries_baltimore.csv', header=TRUE)
data_bt$DATE = as.Date(data_bt$DATE)
```

```{r include=FALSE}
data1_aus = data_aus[(data_aus$DATE >= "1993-01-01") & (data_aus$DATE <= "2022-12-31"),]
high1_aus = data1_aus$TMAX
n1_aus = length(high1_aus)
```

```{r include=FALSE}
data1_sd = data_sd[(data_sd$DATE >= "1993-01-01") & (data_sd$DATE <= "2022-12-31"),]
high1_sd = data1_sd$TMAX
n1_sd = length(high1_sd)
```

```{r include=FALSE}
data1_mn = data_mn[(data_mn$DATE >= "1993-01-01") & (data_mn$DATE <= "2022-12-31"),]
high1_mn = data1_mn$TMAX
n1_mn = length(high1_mn)
```

```{r include=FALSE}
data1_bt = data_bt[(data_bt$DATE >= "1993-01-01") & (data_bt$DATE <= "2022-12-31"),]
high1_bt = data1_bt$TMAX
n1_bt = length(high1_bt)
```

```{r include=FALSE}
time_idx1_aus = 1:n1_aus
date1_aus = data1_aus$DATE
```

```{r include=FALSE}
time_idx1_sd = 1:n1_sd
date1_sd = data1_sd$DATE
```

```{r include=FALSE}
time_idx1_mn = 1:n1_mn
date1_mn = data1_mn$DATE
```

```{r include=FALSE}
time_idx1_bt = 1:n1_bt
date1_bt = data1_bt$DATE
```

# 1. Introduction

Smoothing methods are a popular way to identify the trend(s) in noisy data without having to construct a model of the data-generating process. Temperature data are a particularly appealing application of these methods. Consider this graph of daily high temperatures in Austin, Texas in the late 90s: [1]

```{r}
low=1000
high=2500
ylims = c(10,120)
gg = ggplot(data1_aus[low:high,], aes(x=DATE, y=TMAX)) + geom_line() + labs(title="Austin Daily Highs", x="Date", y="High Temp. (F)") + theme(plot.title = element_text(hjust = 0.5))
print(gg)
```

The graph is visually unappealing due to daily temperature fluctuations. This noise also makes it difficult to generalize and identify trends. As just one example: the winter of 96-97 clearly has the lowest lows of these years, but was that cold spell an aberration or did it truly have the coldest winter?

This project investigates the application of automated smoothing methods to temperature data. Temperature data has several features that complicate the task. These features are detailed in Section {X}, and potential solutions are discussed in subsequent sections. I find that *Super-Smoothing with K-Block Cross-Validation* does a pretty good job of smoothing the data where other methods fail. 

## 1.1 Data

NOAA data request

Austin, San Diego, Minneapolis, Baltimore

Available back into at least the 40s. Truncated to 1993 - 2022 to make computation more manageable.

```{r}
plot_df = data.frame(Daily_High_Temp=c(data1_aus$TMAX, data1_sd$TMAX,
                                       data1_mn$TMAX, data1_bt$TMAX),
                     Date=c(data1_aus$DATE, data1_sd$DATE,
                                       data1_mn$DATE, data1_bt$DATE),
                     City=c(rep("Austin", n1_aus),
                                   rep("S.D.", n1_sd),
                                   rep("Minn.", n1_mn),
                                   rep("Balt.", n1_bt)))
plot_df$Date = as.Date(plot_df$Date)
```

```{r}
gg = ggplot(plot_df, aes(x=Date, y=Daily_High_Temp)) + geom_line() + facet_wrap( ~ City) + labs(title="Daily High Temperatures 1993-2022", y="High Temp. (F)") + theme(plot.title = element_text(hjust = 0.5))
print(gg)
```


```{r}
par(mar=c(5,5,4,2))
boxplot(Daily_High_Temp ~ City, plot_df, cex.lab=1, cex.axis=1)
```

# 2. Kernel Smoothing

---

**Overview:** Kernel smoothers apply a weighted average to the data. They take a *bandwidth* parameter that controls how smooth the output curve is.

---

Smoothing methods try to find a good *smoothing function* that cuts through the noise to give the underlying trend. The value of a smoothing function at a point is generally calculated as some type of weighted average of the values at surrounding points. For instance, the temperature given by the smoothing function on Oct. 1, 1996 is a weighted average of the actual temperatures on and around that date.

Perhaps the simplest smoother is the *constant-span running mean* ([2] p. 366). This smoother is given a parameter $k$ that defines a window size for taking the mean. Suppose we apply the constant-span running mean with $k = 7$ to the temperature data. To calculate the smoothing function on Oct. 1, 1996, we take the mean temperature of the $\frac{k-1}{2} = 3$ days before, the day itself, and the $\frac{k-1}{2} = 3$ days after: in other words, we calculate the mean high temperature from Sep. 28 to Oct. 4. 

While its simplicity is attractive, the constant-span running mean has some problems: the window parameter defines a sharp cutoff for the points it considers, and points at the edge of the window are given as much weight as points in the middle. 

The smoothing method used in this project is known as *kernel smoothing* and tries to fix these problems. A *kernel* is a function that allows kernel smoothers to take truly *weighted* averages. When calculating the smooth function at a given point, the kernel weights observations based on how close they are to that point: the closer the observation, the higher the weight. This project uses a *normal kernel* that assigns weights around a given point in the bell shape of a normal probability distribution.

The effect of distance on weighting in controlled by the all-important *bandwidth* parameter $h$. If $h$ is set to a low value, observations around the given point are weighted very highly, and weights steeply decrease as distance from the point increases. Conversely, if $h$ is set to a high value, close points are given relatively less weight, and weights decrease less steeply with distance.

Suppose again that we're calculating the smoothed temperature on Oct. 1, 1996. The below plot shows how the temperatures on days around this date are weighted given different bandwidth parameters:

```{r}
get_S = function(X, kernel_func, h) {
  # Get smoothing matrix with given kernel and bandwidth
  X_diff = outer(X, X, FUN="-")
  Z = X_diff/h
  S_temp = kernel_func(Z)
  S = S_temp/rowSums(S_temp)
  return(S)
}

kernel_smoother = function(X, Y, kernel_func, h) {
  S = get_S(X, kernel_func, h) # smoothing matrix
  sk_hat = S %*% Y
  return(sk_hat)
}
```

```{r}
data1_aus$s_3  = dnorm((1370-time_idx1_aus)/3)/sum(dnorm((1370-time_idx1_aus)/3))
data1_aus$s_7  = dnorm((1370-time_idx1_aus)/7)/sum(dnorm((1370-time_idx1_aus)/7))
data1_aus$s_14 = dnorm((1370-time_idx1_aus)/14)/sum(dnorm((1370-time_idx1_aus)/14))
data1_aus$s_28 = dnorm((1370-time_idx1_aus)/28)/sum(dnorm((1370-time_idx1_aus)/28))
data1_aus$s_42 = dnorm((1370-time_idx1_aus)/42)/sum(dnorm((1370-time_idx1_aus)/42))
```

```{r}
df_temp = melt(data1_aus[1336:1404,c("DATE","s_3","s_7","s_14","s_28","s_42")], id="DATE")
gg = ggplot(df_temp) + geom_line(aes(x=DATE, y=value, color=variable)) + scale_color_discrete(name="Bandwidth", labels=c(3,7,14,28,42)) + labs(title="Normal Kernel Weights by Bandwidth around 1996/10/01", x="Date", y="Normal Kernel Weight") + theme(plot.title = element_text(hjust = 0.5))
print(gg)
```

Given a bandwidth $h=3$, the smoothed temperature on Oct. 1 mostly depends on the temperatures in a small window around that date. Given a bandwidth $h=28$ or $h=42$, the smoothed temperature on Oct. 1 is giving non-negligible weight to temperatures in August and November.

As seen below, the bandwidth has a significant impact on the smooth:

```{r}
Y_hat1_aus = kernel_smoother(time_idx1_aus, high1_aus, dnorm, 3)
data1_aus$Y_hat_3 = Y_hat1_aus
```

```{r}
Y_hat1_aus = kernel_smoother(time_idx1_aus, high1_aus, dnorm, 42)
data1_aus$Y_hat_42 = Y_hat1_aus
```

```{r}
df_temp = suppressWarnings(melt(data1_aus[(data1_aus$DATE >= "1995-10-01") & (data1_aus$DATE <= "1999-09-30"),c("DATE","TMAX","Y_hat_3","Y_hat_42")], id="DATE"))
gg = suppressWarnings(ggplot(df_temp) + geom_line(aes(x=DATE, y=value, color=variable)) + labs(title="Austin Daily Highs with two smooths",x="Date", y="High Temp. (F)") + scale_color_manual("Bandwidth", values=c("gray","red","blue"), labels=c("N/A", "3", "42")) + theme(plot.title = element_text(hjust = 0.5)))
print(gg)
```

The kernel smoother with $h=3$ overfits the data, following its fluctuations too closely and failing to provide much of a smooth at all. The kernel smoother with $h=42$ underfits the data, failing to fully capture the seasonal extremes in summer and winter.

# 3. Bandwidth Selection

---

**Overview:** Automated bandwidth selection methods scale better, so they're preferred to manual selection when we need to smooth many data sets and / or large data sets.

---

Selecting an appropriate bandwidth is crucially important for getting a good smooth of the data (it's generally viewed as much more important than the choice of kernel function). We can divide methods of bandwidth selection into two broad categories, manual and automated.

## 3.1 Manual Bandwidth Selection

A common way to manually select a bandwidth is by trial and error with visual inspection of the smooths given by different bandwidths. We plot various smooths over the data and see how well they capture the overall trend(s). 

Here are plots of a normal kernel smooth with $h = 14$ for all 4 cities:

```{r}
Y_hat1_aus = kernel_smoother(time_idx1_aus, high1_aus, dnorm, 14)
data1_aus$Y_hat = Y_hat1_aus
```

```{r}
Y_hat1_sd = kernel_smoother(time_idx1_sd, high1_sd, dnorm, 14)
data1_sd$Y_hat = Y_hat1_sd
```

```{r}
Y_hat1_mn = kernel_smoother(time_idx1_mn, high1_mn, dnorm, 14)
data1_mn$Y_hat = Y_hat1_mn
```

```{r}
Y_hat1_bt = kernel_smoother(time_idx1_bt, high1_bt, dnorm, 14)
data1_bt$Y_hat = Y_hat1_bt
```

```{r}
plot_df$Y_hat = c(Y_hat1_aus, Y_hat1_sd, Y_hat1_mn, Y_hat1_bt)
plot_df_trunc = plot_df[(plot_df$Date >= "1995-10-01") & (plot_df$Date <= "1999-09-30"),]
```

```{r}
gg = ggplot(plot_df_trunc) + geom_line(aes(x=Date, y=Daily_High_Temp), color="gray") + geom_line(aes(x=Date, y=Y_hat), color="red") + facet_wrap( ~ City, scales="free") + labs(title="Daily High Temperatures with Normal Kernel Smooth (h=14)", x="Date", y="High Temp. (F)") + theme(plot.title = element_text(hjust = 0.5))
print(gg)
```

By visual inspection, a bandwidth $h=14$ seems to do a pretty good job of smoothing the data, especially for Baltimore and Minnesota.

## 3.2 Automated Bandwidth Selection

A main problem with manual bandwidth selection is that it doesn't scale well in situations with many data sets and / or large data sets. There are reasons for this beyond the obvious time-intensiveness:

 * Many data sets: Even if they're all about the same type of phenomenon, different data sets can have different properties that make different bandwidths more appropriate. We shouldn't assume that a bandwidth that works well on some of the data sets will work well on all of them.  Notice above how the smooth with $h=14$ is noticeably worse for San Diego, containing many small wiggles due to short-term temperature fluctuations. A higher bandwidth could help remove those wiggles and provide a better smooth.
 * Large data sets: It's difficult to visually inspect the smooth over data sets that cover a wide "x-value" range. The above graphs only show 4 years out of 30, so we don't know how well the smooth does in other years.

Methods for automated bandwidth selection are explored in the rest of this project.

# 4. Leave-One-Out Cross-Validation (LOOCV)

--- 

**Overview:** Leave-one-out cross-validation (LOOCV) chooses a bandwidth by simulating how well different bandwidths extend to new data. It falls under the training set / testing set paradigm, with each test set having size 1. LOOCV fails to select good bandwidths for temperature data: given a set of candidate bandwidths, it selects the smallest candidate for each data set.

---

Given a data set of size *n*, leave-one-out cross-validation performs *n* training set / testing set splits. As indicated by the name, each LOOCV testing set contains only one data point that has been left out of the corresponding training set. 

LOOCV chooses among a set of candidate bandwidths by measuring how well smoothers with those bandwidths generalize from the training sets to the test sets. Generalization ability is typically measured using some error function such as the mean-squared error (MSE) used in this project. For a given candidate bandwidth, the LOOCV algorithm iterates through the train / test splits. For a given split, it "trains" a smoother with that bandwidth on the training set and calculates the squared error when applied to the (singleton) test set.^fn.^ The MSE across all splits is returned as the performance measure for that bandwidth. The candidate bandwidth with the smallest MSE is returned as the "optimal" choice.

^fn.^ Fortunately, a mathematical shortcut can be applied so that each smoother doesn't actually have to be trained *n* times. See ([2] p. 371).

I applied LOOCV to each temperature data set, testing candidate bandwidths ranging from $h=1$ to $h=20$ in the integers (as we'll see, testing larger bandwidths is unnecessary). 

```{r}
squared_error = function(u) {
  return(u^2)
}

cross_validate = function(X, Y, kernel_func, error_func, h) {
  # kernel smoother
  n = length(X)
  S = get_S(X, kernel_func, h) # smoothing matrix
  
  sk_hat = S %*% Y # Y_hat
  
  rss = 0
  for (i in 1:n) {
    # components of Givens equation 11.18
    num = Y[i] - sk_hat[i]
    den = 1 - S[i, i]
    # update rss
    rss = rss + error_func(num/den)
  }
  return(rss/n)
}
```

## 4.1. Results

```{r}
# Austin
CVMSE1_aus = c()

for (i in 1:20) {
  bdw = i/1 # i/10
  CVMSE1_aus = c(CVMSE1_aus, cross_validate(
    time_idx1_aus, high1_aus, dnorm, squared_error, bdw))
}
```

```{r}
CVMSE1_sd = c()

for (bdw in 1:20) {
  CVMSE1_sd = c(CVMSE1_sd, cross_validate(
    time_idx1_sd, high1_sd, dnorm, squared_error, bdw))
}
```

```{r}
CVMSE1_mn = c()

for (bdw in 1:20) {
  CVMSE1_mn = c(CVMSE1_mn, cross_validate(
    time_idx1_mn, high1_mn, dnorm, squared_error, bdw))
}
```

```{r}
CVMSE1_bt = c()

for (bdw in 1:20) {
  CVMSE1_bt = c(CVMSE1_bt, cross_validate(
    time_idx1_bt, high1_bt, dnorm, squared_error, bdw))
}
```

The plot below shows the test-set MSE at each candidate bandwidth for each data set. The test-set MSE monotonically increases with bandwidth size, so the LOOCV algorithm selects the smallest candidate ($h=1$) for each city.

In other words, LOOCV fails to select good bandwidths for smoothing these data sets. As seen in Section 2, very small bandwidths barely smooth the data. By selecting the smallest candidate, LOOCV is simply trying to follow the data as closely as possible instead of smoothing it. 

```{r}
plot_df_loocv = data.frame(h=seq(1, 20, by=1),
                           MSE=c(CVMSE1_aus, CVMSE1_sd, 
                                 CVMSE1_mn, CVMSE1_bt),
                           City=c(rep("Austin", 20),
                                   rep("S.D.", 20),
                                   rep("Minn.", 20),
                                   rep("Balt.", 20)))
```

```{r}
gg = ggplot(plot_df_loocv, aes(x=h, y=MSE)) + geom_line() + facet_wrap( ~ City, scales="free") + labs(title="LOOCV MSE by bandwidth", y="Mean Squared Error (MSE)", x="Bandwidth") + theme(plot.title = element_text(hjust = 0.5))
print(gg)
```

# 5. Difficulties Posed By Temperature Data

---

**Overview:** Temperature data has at least two properties that pose difficulties for kernel smoothing and bandwidth selection:

 * Non-Constant Variance
 * Dependent Data

---

Understanding why LOOCV fails to smooth temperature data can help us pick better methods for automated bandwidth selection. 

## 5.1. Non-Constant Variance

The smoothing methods examined up to now assume that the variance of the data is constant across its range. However, temperature data doesn't meet this assumption due to both long and short-term factors. Climate change is an example of a long-term factor affecting temperature variance: as the climate gets further disrupted, we see more temperature extremes and fluctuations. 

The seasons are the primary short-term factor affecting temperature variance. Take another look at some of the Austin data:

```{r}
low=1000
high=2500
gg = ggplot(data1_aus[low:high,], aes(x=DATE, y=TMAX)) + geom_line() + labs(title="Austin Daily Highs", x="Date", y="High Temp. (F)") + theme(plot.title = element_text(hjust = 0.5))
print(gg)
```

We can see from this graph (and I know from personal experience) that temperature variance is much higher in the winter than the summer. Austin winters generally fluctuate between 40F and 80F, often within the span of a couple days, while Austin summers generally stay in the upper 90s to low 100s. 

As a result, different bandwidths are appropriate in different seasons. Higher bandwidths seem better in winter since they'll give smooths that are less affected by the short-term fluctuations. Relatively lower bandwidths seem better in summer since they can better capture the peak heat (usually in August).

## 5.2. Dependent Data

Most smoothing methods work better when deviations from the broader trend in the data are uncorrelated with each other. In the case of temperature data, this would occur if fluctuations around the broader trend were independent from one day to the next. The fluctuations would tend to balance each other out, allowing a smoother to easily cut through the middle of them.

Unfortunately, temperature data isn't like this. When temperatures deviate from the broader trend, they generally do so over the course of several days in a heat wave or cold spell. Abnormally hot days are likely to be followed by abnormally hot days; likewise for abnormally cold days. Consider the highlighted heat wave in Austin during February 1996: 

```{r}
low=1000
high=1450
rect = data.frame(xmin=as.Date("1996-02-18"), xmax=as.Date("1996-02-27"), ymin=-Inf, ymax=Inf)
gg =  ggplot(data1_aus[low:high,], aes(x=DATE, y=TMAX)) + 
  geom_line() + 
  geom_rect(data=rect, aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), fill="yellow", alpha=0.4, inherit.aes=FALSE) + 
  labs(title="Austin Daily Highs", x="Date", y="High Temp. (F)") + 
  theme(plot.title = element_text(hjust = 0.5))
print(gg)
```

Here, the positive deviations from the overall trend lasted about a week. Unless they have a very high bandwidth, smoothers won't be able to "cut through" this heat wave and stick to the broader trend. The prolonged spell of abnormally high temperatures will inevitably pull the smooth upwards, perhaps in dramatic fashion. Looking back at the smoother with $h=14$ in Section 3, this is exactly what happens.

We get more evidence of the dependent nature of temperature data from the autocorrelation (ACF) plots of the residuals from the $h=14$ smooths on each city. The ACF plots display the correlations between different lags of the residuals:  

```{r}
error1_aus = Y_hat1_aus - high1_aus
```

```{r}
error1_sd = Y_hat1_sd - high1_sd
```

```{r}
error1_mn = Y_hat1_mn - high1_mn
```

```{r}
error1_bt = Y_hat1_bt - high1_bt
```

```{r}
par(mar=c(4,4,4,2), mfrow=c(2,2))

acf(error1_aus, lag=20, main="Austin", cex.main=1.5, cex.lab=1)
acf(error1_sd, lag=20, main="San Diego", cex.main=1.5, cex.lab=1)
acf(error1_mn, lag=20, main="Minneapolis", cex.main=1.5, cex.lab=1)
acf(error1_bt, lag=20, main="Baltimore", cex.main=1.5, cex.lab=1)
```

For each city, the residuals are positively correlated with the lag-1 and lag-2 residuals. For Minneapolis, the residuals are even positively correlated with the lag-3 residuals. 

# 6. Super-Smoother

---

**Overview:**

---

```{r}
super_smoother = function(X, Y, kernel_func, H_array,# S_array,
                          error_estimate="loocv", lko_window=2, error_bdw=NA) {
  # H_array is the set of candidate bandwidths
  m = length(H_array)
  n = length(X)
  
  k = (m+1)/2
  if (is.na(error_bdw)) {
    error_bdw = H_array[k]
  }
  
  # fixed "outer" smoother
  S_star_h = get_S(X, kernel_func, error_bdw)
  
  # get performance measures at each point for each candidate bandwidth
  smooth_list = vector(mode="list", length=m)
  p_hat_list = vector(mode="list", length=m) # smoothed performance measure
  for (i in 1:m) {
    h = H_array[i]
    
    S_h = get_S(X, kernel_func, h) # smoothing matrix
    #S_h = S_array[i]
    
    sk_hat_h = S_h %*% Y
    smooth_list[[h]] = sk_hat_h
    
    if (error_estimate=="loocv") {
      error_h = (Y - sk_hat_h)/(1 - diag(S_h)) 
    }
    else if (error_estimate=="lkocv") {
      error_h = c()
      for (j in 1:n) {
        yj = Y[j]
        Y_temp = Y
        Y_temp[max((j-lko_window),1):min((j+lko_window),n)] = yj
        sk_hatj = dot(c(S_h[1:n,j]), Y_temp)
        
        num = Y[j] - sk_hatj
        den = 1 - sum(S_h[max((j-lko_window),1):min((j+lko_window),n), j])
        error_j = num/den
        
        error_h = c(error_h, c(error_j))
      }
    }

    g_h = abs(error_h)
    
    # smooth errors
    p_hat_h = S_star_h %*% g_h
    p_hat_list[[i]] = p_hat_h
  }
  
  p_hat_matrix = matrix(unlist(p_hat_list), ncol=m)
  smooth_matrix = matrix(unlist(smooth_list), ncol=m)
  
  # pick best bandwidths
  min_indices = argmin(p_hat_matrix)
  h_min = c()
  for (i in 1:n) {
    h_min = c(h_min, H_array[min_indices[i]])
  }
  
  # smooth best bandwidths
  smoothed_best_h = S_star_h %*% h_min
  smoothed_best_h = round(smoothed_best_h, 6) # floats would go below min(H) sometimes
  
  # use bandwidths to predict
  sk_hat = c()
  for (i in 1:n) { 
    h_i = smoothed_best_h[i]
    h_minus = max(H_array[H_array <= h_i])
    h_plus = min(H_array[H_array >= h_i])
    
    # interpolate higher and lower bandwidths
    if (h_minus==h_plus) {
      p = 0
    }
    else {
      p = (h_i - h_minus)/(h_plus - h_minus)
    }
    
    sk_i = p*smooth_list[[h_plus]][i] + (1-p)*smooth_list[[h_minus]][i]
    sk_hat = c(sk_hat, c(sk_i))
  }
  return(list(sk_hat, h_min, smoothed_best_h))
}
```

# 7. K-Block Cross-Validation

---

**Overview:**

---

# 8. References

[1] https://www.ncei.noaa.gov/cdo-web/

[2] Givens and Hoeting (2012), Computational Statistics.